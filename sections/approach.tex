



\begin{table}
\caption{EMBERS system statistics}
 \centering
 \begin{tabular}{|l|l|l|l|l|}
 \hline
 Archived data     & 12.4 TB                  \\ \hline
 Archive size & ca. 3 billion messages   \\ \hline
 Data throughput   & 200-2000 messages/sec  \\ \hline
 Daily ingest & 15 GB \\ \hline
 System memory & 50 GB \\ \hline
 System core & 16 vCPUs \\ \hline
 System output & ca. 40 warnings/day \\ \hline
\end{tabular}
\label{tab:stats}
\end{table}


The general approach we adopt is to identify open-source documents
that appear to indicate civil-unrest event planning, to extract
relevant information from identified documents and use that as the
basis for a structured warning about the planned event.  We ingest a
wide array of open-soucre textual documents, from news articles and
blogs, to Twitter posting (tweets) as well as Facebook Event pages.
The textual documents are subjected to linguistic analysis; candidate
documents are identified using a a list of phrases associated with
event planning; date and location information is extracted from the
text and used as the basis for the warning.  Ecah of these processing
steps is outlined below.



\subsection{Linguistic Preprocessing}

As part of the general streaming architecture of the EMBERS system,
all textual input (e.g., tweets, news articles, blog postings) is
subjected to shallow linguistic processing prior to analysis.  This
involves identifying the language of the document, distinguishing the
the words (tokenization), normalizing words for inflection
(lemmatization), and identifying expressions referring to people,
places, dates and other entities and classifying them (named entity extraction). As our
data set is multilingual, with Spanish, Portuguese and English
predominating, we use a suite of multilingual commercial tools for
this processing.\footnote{BASIS Technology's Rossette Linguistic Platform}

This linguistic preprocessing serves as input to subsequent deeper semantic analysis in which 
date expressions are normalized and deindexed, the geographic focus of the text identified.
This processing chain is illustrated in Fig.~\ref{fig:enrichment}.

\begin{figure}
    \includegraphics[width=0.5\textwidth]{enrichment}
    \caption{Message Enrichment}
    \label{fig:enrichment}
\end{figure}

Date processing is particularly crucial to the identification of
future oriented statements. We use the TIMEN \cite{LlorensDGS12} date
normalization package to normalize and deindex expressions referring
to days in English, Spanish and Portuguese.  This system makes use of
meta-data, such as the day of publication, and other information about
the linguistic context of the date expression to determine for each date expression,
what day (or week, month or year) it refers to.  For example in a tweet produced on June 10, 2014, the occurrence of 
the term {\em Friday} used in a future-tense sentence {\em We'll get together on Friday} will be interpreted as July 13, 2014.
Each expression identified as a date by the RLP preprocessor is normalized in this way.

Finally, messages are geocoded with a specification of the geographical focus of the text---specified as a city, state, country tripple.
We make use of different geocoding methodologies for geo-coding news/blogs and for short messages such as twitter.


\subsection{Geo-Coding}
\label{subsection:geocoding}



\subsubsection{Twitter}

For tweets, the geo-focus of the message is generated by a fairly
simple set of heuristics.  In particular, Twitter
geocoding is achieved by first considering the most reliable but least
available source, the geotag of the tweet itself (this is available
for about 10% of our sample from Twitter). This provide an exact
geographic locations that can be reverse geocoded into a place names
and this used as the geo-focus. We find the nearest geo-coded point in
our extended gazetteer (using the KD-Tree algorithm) for this
purpose. If the tweet is not geocoded, we consider Twitter ``places''
metadata and use place names present in these metadata fields to
geocode the place names into geographical coordinates.  Finally, if
none of this is available, we consider the text fields contained in
the user profile (location, description) as well as the tweet text
itself to find mentions of relevant locations.  Additional toponym disambiguiation heuristics are used to
identify the actual referent of the mention.

This simple set of heuristics achieves accuracy of ... 
(GRAHAM: RESULTS OF GEOCODING STUDY HERE)

\subsubsection{Facebook}
Similar methods are used to geocode event data extracted from Facebook Events pages.   Since only Facebook Events that have a venue are used and a venue of a Facebook Event generally contains a latitude, longitude, and physical address information identifying the locatino is a fairly trivial task.  In cases where only latitude and longitude are given we apply reverse-geocoding mechanisms similar to those used for Twitter.


\subsubsection{News/Blogs}

For longer articles such as news articles, the geo-focus of the message is identified using much more complex methods
To extract the protest location from news articles, we use \emph{probabilistic soft logic} (PSL) described in ~\ref{section:PSL} to build a model that performs robust, probabilistic inference given noisy signals. PSL takes a set of weighted, logic-like rules and converts them into a continuous probability distribution over the unknown truth values of logical facts. These truth values in PSL are relaxed into the $[0,1]$ interval. We use this mechanism to build a model that infers the semantic location of an article by weighing evidence coming from the Basis entity extractions and information in the World Gazatteer. 

The primary rules in the model encode the effect that Basis-extracted location strings that match to gazatteer aliases are indicators of the article's location, whether they be country, state, or city aliases. Each of these implications is conjuncted with an prior for ambiguous, overloaded aliases that is proportional to the population of the gazetteer location. For example, if the string ``Los Angeles'' appears in the article, it could refer to either Los Angeles, California, or Los \'{A}ngeles in Argentina or Chile. Given no other information, our model would infer a higher truth value for the article referring to Los Angeles, California, because it has a much higher population than the other options. 

\begin{flalign*}
    ENTITY&(L, location) \softand REFERSTO(L, locID) &\\
                        &\rightarrow PSLLOCATION(Article, locID) &
\end{flalign*}


\begin{flalign*}
    ENTITY&(C, location) \softand IsCountry(C) &\\
                        &\rightarrow ArticleCountry(Article, C) &
\end{flalign*}


\begin{flalign*}
    ENTITY&(S, location) \softand IsState(S)&\\
                            &\rightarrow ArticleCountry(Article, S)&
\end{flalign*}

The secondary rules, which are given half the weight of the primary rules, perform the same mapping of extracted strings to gazetteer aliases, but for extracted persons and organizations. Strings describing persons and organizations often include location clues (e.g., ``mayor of Buenos Aires''), but intuition suggests the correlation between the article's location and these clues may be lower than with location strings. 

\begin{flalign*}
    ENTITY&(O, organization) \softand REFERSTO(O, locID)&\\
                            &\rightarrow PSLLOCATION(Article, locID) &
\end{flalign*}


\begin{flalign*}
    ENTITY&(O, organization) \softand IsCountry(O)&\\
        &\rightarrow ArticleCountry(Article, O)&
\end{flalign*}


\begin{flalign*}
    ENTITY&(O, organization) \softand IsState(O)&\\
          &\rightarrow ArticleCountry(Article, O) &
\end{flalign*}
Finally, the model includes rules and constraints to require consistency between the different levels of geolocation, making the model place higher probability on states with its city contained in its state, which is contained in its country. As a post-processing step, we enforce this consistency explicitly by using the inferred city and its enclosing state and country, but adding these rules into the model makes the probabilistic inference prefer consistent predictions, enabling it to combine evidence at all levels.

\begin{flalign*}
    PSLLOCATION&(Article, locID) \softand Country(locID, C)&\\
               &\rightarrow ArticleCountry(Article, C)&
\end{flalign*}


\begin{flalign*}
    PSLLOCATION&(Article, locID) \softand Admin1(locID, S)&\\
               &\rightarrow ArticleState(Article, S)&
\end{flalign*}


\begin{figure*}
    \includegraphics[width=\textwidth]{psl_pipeline}
    \caption{Red circles denote named entities identified as locations and blue denotes other types of entities. The article is reported from Weston Florida US and talks about the recent increase of venezuelan population in the US compared to other Latin American Nations like Cuba etc.\sathappanc{TODO: Replace with a better protest example}}
    \label{fig:psl_example}
\end{figure*}

(GRAHAM: DO WE WANT TO SUMMARIZE SOME RESULTS HERE TOO?)

%\input{sections/psl.tex}

\iffalse Most news articles and blog posts mention multiple locations, e.g.,
the location of reporting, the location of the incident, and locations corresponding
to the hometown of the newspaper. We developed a probabilistic reasoning
engine using probabilistic soft logic (PSL)
to infer the most likely city, state and country which is the main geographic focus the article.The PSL geocoder combines various types of evidence, such as named entities
such as locations, persons, and organizations identified by RLP, as
well as common names and aliases and populations of known
locations. These diverse types of evidence are used in weighted rules
that prioritize their influence on the PSL model's location
prediction. For example, extracted location tokens are strong
indicators of the content location of an article, while organization
and person names containing location names are weaker but still
informative signals; the rules corresponding to these evidence types
are weighted accordingly.

The methodology is similar to {\em Web-a-where: Geo-Tagging Web Content}.
\fi 

\begin{figure}
    \includegraphics[width=0.5\textwidth]{rssdistribution}
    \caption{Rate of Arrival of News/Blogs}
    \label{fig:rssdistribution}
\end{figure}




\begin{figure*}
\includegraphics[width=\textwidth]{pp_pipeline}
\caption{A diagram showing various steps of the Model}
\end{figure*}

\subsection{Phrase filtering}

Each input message (document) is searched for the presence of one or
more key phrases in a list of phrases indicative of an article's being
about a planned civil unrest event.  Articles which do match are sent on for further processing,
and those that do not are ignored, again, as part of the streaming infrastructure.

The list of key phrases indicating civil unrest planning was obtained
in a semi-automatic manner, as detailed in the following section. The
phrases are general rules for matching, rather than literal string
sequences, typically consisting of two or more word lemmas, a language
specification and a separation threshold. This separation threshhold is automatically learned
in the learning phrase described below.  It was found that this kind of multi-word key phrases was found more accurate than simple
keywords for extracting events of interest from the data stream.

The presence of a keyphrase is checked by searching for the presence of
individual lemmas of the keyphrase within the same sentence separated
by at most a number of word that is fewer than the separation threshold.  
This method allows for linguistically sophisticated and flexible matching, so, for example,
they keyphrase [{\em plan protest}, 4, English] would match the sentence
{\em The students are planning a couple big protests tomorrow} in an input document.


\subsubsection{Phrase list development}
\label{sec:phraselearning}

The set of key phrases was tailored (slightly) the the genre of the
input. In particular different phrases were used to identify relevant
news articles and blogs from those used to filter Tweets.  The lists
themselves were generated semi-audomatically.

Initially, a few seed phrases were obtained manually
with the help of subject matter experts.
\sathappanc{Jaime's Text --> edited by graham}
An analysis of news reports for planned protests in the print media helped create a
minimum set of words to use in the query.  We choose four nouns from
the basic query that is used predominantly to indicate a civil unrest
in the print media - {\em demonstration, march, protest and
  strike}. We translated them into Spanish and Portuguese, including
synonyms.  We then combined these with future-oriented verbs - {\em to organize}, {\em to prepare}, {\em to
plan}, {\em to announce}, etc. For twitter, shorter phrases were identified, and these had
a more direct call for action, for example, {\em marchar}, {\em manhã de mobilização}, {\em
  vamos protestar}, {\em huelga}.

To generalize this set of phrases, the the phrases were then parsed
using a dependency parser \cite{freeling} and the grammatical
relationship between the core the nominal focus word (e.g., {\em
  protest}, {\em manifestación}, {\em huelga}) and any accompanying
word (e.g., {\em plan}, {\em call}, {\em anunciar}) was
extracted. These grammatical relations were used as extraction
patterns as in \cite{riloff2003learning} to learn more phrases from a
corpora of sentences extracted from the data stream of interest
(either news/blogs or tweets). This corpus consists of sentences that
contained any one of the nominal focus words and also had mentions of a
future date.

The phrase learning is shown in Fig.~\ref{fig:phraselearning}

The set of learned phrases, is then cleansed by an expert to get the final set of key phrases.
Using this approach, we learned 112 phrases for news articles andblogs and 156 for tweets.

\begin{figure}
\caption{An Example of Phrase Learning}
\includegraphics[width=0.5\textwidth]{figures/phraseLearning}
\label{fig:phraselearning}
\end{figure}

\subsection{Warning Generation}

After being subject to the preprocessing steps, above, all documents
that are identified as cointaning a key phrase are further filtered by
searching for the presence a future date in the passage contaiing the
key phrase and for the existence of an identified geographical focus for the document.
Those documents that meet all these critera are used as the basis for a warning about a
planned civil unrest event. Twitter postings are only used as the basis for a warning
if the tweet is re-tweeted at least five times.

The warning is generated for the date indicated by the future date
expression, the location which is the geographical focus of the
document.  In addition, for news articles and blog documents, the
Event Type information and Population information is derived from the
use of a text-based Naive Bayes classifier.  This classifier is
trained on the Gold Standard Resource and makes use of unigram and bi-gram word features.
For Twitter, since there is very little text in an individual tweet, the
event-type and population are based on based on prior likelihood for that location in the GSR.

In the case of Facebook, a Facebook-Event is considered to be good evidence for an alert if
there are more attendees for the event than rejects.  The date and
location are read off the Event page itself, and the population and event type are also based on priors.




