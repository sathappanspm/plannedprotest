%!TEX root = ../plannedprotest_IAAI.tex

%\iffalse
%To extract the protest location from news articles, we use \emph{probabilistic soft logic} (PSL) \cite{broecheler:uai10} to build a model that performs robust, probabilistic inference given noisy signals. PSL takes a set of weighted, logic-like rules and converts them into a continuous probability distribution over the unknown truth values of logical facts. These truth values in PSL are relaxed into the $[0,1]$ interval. We use this mechanism to build a model that infers the semantic location of an article by weighing evidence coming from the Basis entity extractions and information in the World Gazatteer. 
%
%The primary rules in the model encode the effect that Basis-extracted location strings that match to gazatteer aliases are indicators of the article's location, whether they be country, state, or city aliases. Each of these implications is conjuncted with an prior for ambiguous, overloaded aliases that is proportional to the population of the gazetteer location. For example, if the string ``Los Angeles'' appears in the article, it could refer to either Los Angeles, California, or Los \'{A}ngeles in Argentina or Chile. Given no other information, our model would infer a higher truth value for the article referring to Los Angeles, California, because it has a much higher population than the other options. 
%
%The secondary rules, which are given half the weight of the primary rules, perform the same mapping of extracted strings to gazetteer aliases, but for extracted persons and organizations. Strings describing persons and organizations often include location clues (e.g., ``mayor of Buenos Aires''), but intuition suggests the correlation between the article's location and these clues may be lower than with location strings. 
%
%Finally, the model includes rules and constraints to require consistency between the different levels of geolocation, making the model place higher probability on states with its city contained in its state, which is contained in its country. As a post-processing step, we enforce this consistency explicitly by using the inferred city and its enclosing state and country, but adding these rules into the model makes the probabilistic inference prefer consistent predictions, enabling it to combine evidence at all levels.
%\fi

In this section, we briefly describe probabilistic soft logic (PSL)~\cite{kimmig2012short}, a key
component of our geocoding strategy.
PSL is a framework for collective probabilistic reasoning on relational domains.
%%PSL models have been developed in various domains, including 
%collective classification,
%ontology alignment,
%personalized medicine,
%opinion diffusion,
%trust in social networks, and graph
%summarization.
PSL represents the domain of interest as logical atoms.
It uses first order logic rules to capture the dependency structure of the domain, based on which it builds a joint probabilistic model over all atoms.
Instead of hard truth values of $0$ (false) and $1$ (true), PSL uses soft truth values relaxing the truth vlaues to the interval $[0,1]$.
The logical connectives are adapted accordingly.
%This makes it easy to incorporate similarity or distance functions.

User defined \emph{predicates} are used to encode the relationships and attributes and \emph{rules} capture the  dependencies and constraints.
%Each rule's antecedent is a conjunction of atoms and its consequent is a dis-junction. 
The rules can also be
labeled with non-negative weights which are used during the inference process.
The set of predicates and weighted rules thus make up a PSL program where known truth values of ground atoms derived from observed data and unknown truth values for the remaining atoms are learned using the PSL inference.

Given a set of atoms 
$\ell = \{\ell_1,\ldots,\ell_n\}$,
an interpretation defined as 
$I : \ell \rightarrow [0,1]^n$
is a mapping from atoms to soft truth values.
PSL defines a probability distribution over all such interpretaions such that those that satisfy more ground rules are more probable.
\emph{Lukasiewicz t-norm} and its corresponding co-norm are used for defining relaxations of the logical AND and OR respectively to determine the degree to which a ground rule is satisfied.
Given an interpretation $\mathit{I}$, PSL defines the formulas for the relaxation of the logical conjunction ($\wedge$), disjunction ($\vee$), and negation ($\neg$) as follows:
\begin{align*}
\ell_1 \softand \ell_2 &= \max\{0, I(\ell_1) + I(\ell_2) - 1\},\\
\ell_1 \softor \ell_2 &= \min\{I(\ell_1) + I(\ell_2), 1\},\\
\softneg l_1 &= 1 - I(\ell_1),
\end{align*}  

The interpretation $\mathit{I}$ determines whether the rules are
satisfied. A rule $\mathit{r} \equiv \mathit{r_{body}} \rightarrow
\mathit{r_{head}} $  is satisfied if and only if the truth value of head
is at least that of the body. Otherwise, PSL uses a \emph{distance to
satisfaction}, which measures the degree to which this condition is
violated
\begin{center} 
 $\mathit{d_r}(\mathit{I}) =$ max\{0,$\mathit{I(r_{body})} - \mathit{I(r_{head})}$\}.
 \end{center}

PSL then induces a probability distribution over possible interpretations $\mathit{I}$ over the given set of ground atoms $\mathit{l} $ in the domain. 
If $\mathit{R}$ is the set of all ground rules that are instances of a rule from the system and uses only the atoms in  $\mathit{I}$ then,
the probability density function $\mathit{f}$ over $\mathit{I}$ is defined as
\begin{equation}
\label{eq:contimn1}
    f (I) = \frac{1}{Z} \exp \left(-\sum_{r\in R} \lambda_r (d_r(I))^p \right)
\end{equation}
\begin{equation}
\label{eq:contimn2}
	Z = \int_{I} \exp \left( -\sum_{r\in R} \lambda_r (d_r(I))^p \right)
\end{equation}

where $\lambda_r$ is the weight of the rule $r$, $Z$ is a normalization
constant, and $p \in {1, 2}$ provides a choice between linear or
quadratic loss functions, which produce different modeling behavior. PSL
further allows inclusion of linear equality and inequality constraints,
which enable modeling of functional constraints on the domains and
ranges of predicates. 

The probability distribution in Equation (1) is an example of a
\emph{hinge-loss Markov random field} \cite{bach:uai13}, which has a
form of energy function that makes inference of the most probable
explanation an efficient convex optimization. The expressiveness of PSL
and the efficiency of inference in its models allows us to encode
dependencies between various aspects of geolocation that are jointly
inferred.

%where~$\lambda_r$ is the weight of the rule~$r$, $Z$ is the continuous
%version of the normalization constant used in discrete Markov random
%fields, and ~$p \in \{1, 2\}$ provides a choice between two different
%loss functions, linear and quadratic.  The values of the atoms can be
%further restricted by providing linear equality and inequality
%constraints allowing one to encode functional constraints from the
%domain. 
%
%PSL provides for two kinds of inferences: (a) most probable explanation
%and (b) calculation of the marginal distributions.  In the MPE
%inference given a partial interpretation with grounded atoms based on
%observed evidence, the PSL program infers the truth values for the
%unobserved atoms satisfying the most likely interpretation.  In the
%second setting, given ground truth data for all atoms we can learn the

%weights for the rules in our PSL program.
